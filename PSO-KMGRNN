# coding:utf-8
import random
import pandas as pd
import numpy as np
np.random.seed(10)
import matplotlib.pyplot as plt
import csv
import math
from sklearn.metrics import mean_squared_error

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def normalize(data, axis=-1, p=2):
    lp_norm = np.atleast_1d(np.linalg.norm(data, p, axis))
    lp_norm[lp_norm == 0] = 1
    return data / np.expand_dims(lp_norm, axis)

def euclidean_distance(one_sample, data):
    one_sample = one_sample.reshape(1, -1)
    data = data.reshape(data.shape[0], -1)
    distances = np.power(np.tile(one_sample, (data.shape[0], 1)) - data, 2).sum(axis=1)
    return distances

class Kmeans():
    def __init__(self, k=2, max_iterations=500, varepsilon=0.0001):
        self.k = k
        self.max_iterations = max_iterations
        self.varepsilon = varepsilon

    def init_random_centroids(self, data):
        n_samples, n_features = np.shape(data)
        centroids = np.zeros((self.k, n_features))
        for i in range(self.k):
            random.seed(10)
            centroid = data[np.random.choice(range(n_samples))]
            centroids[i] = centroid
        print('初始聚类中心')
        print(centroids)
        return centroids

    def _closest_centroid(self, sample, centroids):
        distances = euclidean_distance(sample, centroids)
        closest_i = np.argmin(distances)
        return closest_i

    def create_clusters(self, centroids, data):
        n_samples = np.shape(data)[0]
        clusters = [[] for _ in range(self.k)]
        for sample_i, sample in enumerate(data):
            centroid_i = self._closest_centroid(sample, centroids)
            clusters[centroid_i].append(sample_i)
        return clusters

    def update_centroids(self, clusters, data):
        n_features = np.shape(data)[1]
        centroids = np.zeros((self.k, n_features))
        for i, cluster in enumerate(clusters):
            centroid = np.mean(data[cluster], axis=0)
            centroids[i] = centroid
        print('更新聚类中心')
        print(centroids)
        print('--------------------------------------------------')
        return centroids

    def get_cluster_labels(self, clusters, data):
        y_pred = np.zeros(np.shape(data)[0])
        for cluster_i, cluster in enumerate(clusters):
            for sample_i in cluster:
                y_pred[sample_i] = cluster_i
        return y_pred

    def predict(self, data, list):
        centroids = self.init_random_centroids(data)

        for _ in range(self.max_iterations):
            clusters = self.create_clusters(centroids, data)
            former_centroids = centroids

            centroids = self.update_centroids(clusters, data)

            diff = centroids - former_centroids
            if diff.any() < self.varepsilon:
                break
        print('最终聚类中心')
        print(centroids)
        list.extend(centroids)
        return self.get_cluster_labels(clusters, data)

def minmax(train):
    n = len(train[0])
    list1 = []
    for j in range(n):
        colmax = max(train[:, j])
        colmin = min(train[:, j])
        list1.append(colmin)
        list1.append(colmax)
    print(list1)
    return list1

def mapminmax(con_data, list1):
    m = len(con_data)
    n = len(con_data[0])
    change_data = np.zeros((m, n))
    for j in range(n):
        for i in range(m):
            change_data[i, j] = 2 * (con_data[i, j] - list1[2 * j]) / (list1[2 * j + 1] - list1[2 * j]) - 1
    return change_data

def outputmapminmax(con_data, list1):
    m = len(con_data)
    n = 1
    change_data = np.zeros((m, n))
    for i in range(m):
        change_data[i] = 2 * (con_data[i] - list1[-2]) / (list1[-1] - list1[-2]) - 1
    return change_data

def distance(trainX, testX, sigmax, sigmay, sigmaz, sigma):
    p = len(testX)
    m = len(trainX)
    Euclidean_DF_guass = np.zeros((p, m))
    for i in range(p):
        for j in range(m):
            tempx = (testX[i, 0] - trainX[j, 0]) * (testX[i, 0] - trainX[j, 0])
            tempy = (testX[i, 1] - trainX[j, 1]) * (testX[i, 1] - trainX[j, 1])
            tempz = (testX[i, 2] - trainX[j, 2]) * (testX[i, 2] - trainX[j, 2])
            tempp = np.sum((testX[i, 3:] - trainX[j, 3:]) * (testX[i, 3:] - trainX[j, 3:]))
            Euclidean_DF_guass[i, j] = np.exp((-tempx / (2 * sigmax * sigmax)) + (-tempy / (2 * sigmay * sigmay)) +
                                              (-tempz / (2 * sigmaz * sigmaz)) + (-tempp / (2 * sigma * sigma)))
    return Euclidean_DF_guass

def sum_layer(Gauss, outputn_train1):
    m = len(Gauss)
    l = len(Gauss[0])
    n = len(outputn_train1[0])
    sum_out = np.zeros((m, n + 1))
    for i in range(m):
        sum_out[i, 0] = np.sum(Gauss[i, :])
    for i in range(m):
        total = 0.0
        for t in range(l):
            total += Gauss[i, t] * outputn_train1[t, 0]
        sum_out[i, 1] = total
    return sum_out

def output_layer(sum_out):
    m = len(sum_out)
    n = len(sum_out[0])
    prediction_result = np.zeros((m, n - 1))
    for j in range(m):
        if (sum_out[j, 0] == 0):
            prediction_result[j, 0] = 0
        else:
            prediction_result[j, 0] = sum_out[j, 1] / sum_out[j, 0]
    return prediction_result

def reverseminmax(predict_data1, bmin, bmax):
    m = len(predict_data1)
    reverse_data = np.zeros((m, 1))
    for i in range(m):
        reverse_data[i, 0] = (predict_data1[i, 0] + 1) * (bmax - bmin) / 2 + bmin  # 反归一化
    return reverse_data

def get_mse(records_real, records_predict):
    if len(records_real) == len(records_predict):
        return sum([(x - y) ** 2 for x, y in zip(records_real, records_predict)]) / len(records_real)
    else:
        return None

def get_rmse(records_real, records_predict):
    mse = get_mse(records_real, records_predict)
    if mse:
        return math.sqrt(mse)
    else:
        return None

def get_mae(records_real, records_predict):
    if len(records_real) == len(records_predict):
        return sum([abs(x - y) for x, y in zip(records_real, records_predict)]) / len(records_real)
    else:
        return None

def R2(output_test, grnn_output):
    SSR = np.sum((grnn_output - np.mean(output_test)) ** 2)
    SST = np.sum((output_test - np.mean(output_test)) ** 2)
    # SSE = np.sum((grnn_output - output_test) ** 2)
    R2 = SST / SSR
    return R2

def mape(records_real, records_predict):
    return np.mean(np.abs((records_predict - records_real) / records_real)) * 100

def data_split(full_list, ratio, shuffle=False):
    n_total = len(full_list)
    offset = int(n_total * ratio)
    if n_total == 0 or offset < 1:
        return [], full_list
    if shuffle:
        random.shuffle(full_list)
    sublist_1 = full_list[:offset]
    sublist_2 = full_list[offset:]
    return sublist_1, sublist_2

# ----------------------PSO参数设置---------------------------------
class PSO():
    def __init__(self, pN, dim, max_iter, w):
        self.w = w
        self.wmax = 0.9
        self.wmin = 0.1
        self.r1 = 0.6
        self.r2 = 0.3
        self.Xmax = 1
        self.Xmin = 0
        self.pN = pN  # 粒子数量
        self.dim = dim  # 搜索维度
        self.max_iter = max_iter  # 迭代次数
        self.X = np.zeros((self.pN, self.dim))  # 所有粒子的位置和速度
        self.V = np.zeros((self.pN, self.dim))
        self.pbest = np.zeros((self.pN, self.dim))  # 个体经历的最佳位置和全局最佳位置
        self.gbest = np.zeros((1, self.dim))
        self.p_fit = np.zeros(self.pN)  # 每个个体的历史最佳适应值
        self.fit = 1e10  # 全局最佳适应值

    def getweight(self, iter):
        w = self.wmax - (self.wmax - self.wmin) * np.sqrt(iter / self.max_iter)
        return w

    def getc(self, iter):
        c1 = 2 * np.sqrt(np.cos((np.pi / 2) * (iter / self.max_iter)))
        c2 = 2 * np.sqrt(1 - (np.cos((np.pi / 2) * (iter / self.max_iter))))
        return c1, c2

    # ---------------------目标函数Sphere函数-----------------------------
    def function(self, X):
        Euclidean_DF = distance(inputn_train, inputn_test, X[0], X[1], X[2], X[3])
        sum_out = sum_layer(Euclidean_DF, outputn_train)
        prediction_result = output_layer(sum_out)
        grnn_output = reverseminmax(prediction_result, bmin, bmax)
        return mean_squared_error(output_test, grnn_output)

    # ---------------------初始化种群----------------------------------
    def init_Population(self):
        for i in range(self.pN):
            for j in range(self.dim):
                self.X[i][j] = random.uniform(0, 1)
                self.V[i][j] = random.uniform(0, 1)
            self.pbest[i] = self.X[i]
            tmp = self.function(self.X[i])
            self.p_fit[i] = tmp
            if tmp < self.fit:
                self.fit = tmp
                self.gbest = self.X[i]

    # ----------------------更新粒子位置----------------------------------
    def iterator(self):
        fitness = []
        for t in range(self.max_iter):
            weight = self.getweight(t)
            c1, c2 = self.getc(t)
            for i in range(self.pN):  # 更新gbest\pbest
                temp = self.function(self.X[i])
                if temp < self.p_fit[i] and self.X[i][0] < self.Xmax and self.X[i][0] > self.Xmin \
                        and self.X[i][1] < self.Xmax and self.X[i][1] > self.Xmin \
                        and self.X[i][2] < self.Xmax and self.X[i][2] > self.Xmin \
                        and self.X[i][3] < self.Xmax and self.X[i][3] > self.Xmin:  # 更新个体最优
                    self.p_fit[i] = temp
                    self.pbest[i] = self.X[i]
                    if self.p_fit[i] < self.fit:  # 更新全局最优
                        self.gbest = self.X[i]
                        self.fit = self.p_fit[i]
            for i in range(self.pN):
                self.V[i] = weight * self.V[i] + c1 * self.r1 * (self.pbest[i] - self.X[i]) + \
                            c2 * self.r2 * (self.gbest - self.X[i])
                self.X[i] = self.X[i] + self.V[i]
            print(weight)
            fitness.append(self.fit)
            print("sigma", self.X[0], end=" ")
            sigma_list.append(self.X[0])
            fit_list.append(self.fit)
            print('均方', self.fit)  # 输出最优值
        return fitness

def main():
    loan_data = pd.DataFrame(pd.read_csv('train.csv', header=0))

    print(loan_data.head())
    print(loan_data.columns)

    data = np.array(loan_data[['X', 'Y', 'Z']])
    data_2 = np.array(loan_data[['X', 'Y', 'Z', 'P1', 'P2', 'P3']])

    clf = Kmeans(k=3)
    list = []
    y_pred = clf.predict(data, list)
    print('聚类结果')
    print(y_pred)
    print('--------------------------------------------------')
    print('聚类中心')
    print(list)

    s = 0
    for i in y_pred:
        if i == 0:
            s += 1
        else:
            s0 = s
    print('第0类个数')
    print(s0)
    a = 0
    for i in y_pred:
        if i == 1:
            a += 1
        else:
            s1 = a
    print('第1类个数')
    print(s1)
    b = 0
    for i in y_pred:
        if i == 2:
            b += 1
        else:
            s2 = b
    print('第2类个数')
    print(s2)
    color_list = ['green', 'black', 'purple', 'orange', 'yellow', 'red']

    ax1 = plt.subplot(111, projection='3d')

    ax1.scatter(data[y_pred == 0][:, 0], data[y_pred == 0][:, 1], data[y_pred == 0][:, 2], c='r', marker='^', s=15,
                label="第一类")
    ax1.scatter(data[y_pred == 1][:, 0], data[y_pred == 1][:, 1], data[y_pred == 1][:, 2], c='b', marker='x', s=15,
                label="第二类")
    ax1.scatter(data[y_pred == 2][:, 0], data[y_pred == 2][:, 1], data[y_pred == 2][:, 2], c='green', marker='v', s=15,
                label="第三类")

    plt.legend()

    for item in list:
        ax1.scatter(item[0], item[1], item[2], s=200, c=random.choice(color_list))
        hes = ['X', 'Y', 'Z', 'P1', 'P2', 'P3', '聚类类别']

        with open("train_after3C.csv", "w", encoding="GB18030", newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(hes)
            for i in range(len(data)):
                writer.writerow([data_2[i][0], data_2[i][1], data_2[i][2], data_2[i][3], data_2[i][4], data_2[i][5],
                                 str(y_pred[i])[0]])

    plt.legend(loc='upper left')
    ax1.set_zlabel('Z')
    ax1.set_ylabel('Y')
    ax1.set_xlabel('X')

    plt.show()

if __name__ == "__main__":
    main()

for i in range(3):
    df = pd.read_csv('data3343_after3C.csv', encoding='gbk')
    read_data = df[df['聚类类别'] == i]

    sublist_1,sublist_2 = data_split(read_data, 0.9)

    dt = pd.DataFrame(sublist_1)
    dt1 = pd.DataFrame(sublist_2)

    dt = dt.iloc[0:, 0:dt.shape[1] - 1].values
    dt1 = dt1.iloc[0:, 0:dt1.shape[1] - 1].values

    dt = pd.DataFrame(dt)
    dt1 = pd.DataFrame(dt1)

    # 步骤一 读入数据
    num1 = dt
    num2 = dt1

    m = num1.shape[1]
    n = num2.shape[1]

    input_train = num1.iloc[0:, 0:m - 1].values
    output_train = num1.iloc[0:, m - 1].values
    input_test = num2.iloc[0:, 0:n - 1].values
    output_test = num2.iloc[0:, n - 1].values

    input_train[np.isnan(input_train)] = 0
    output_train[np.isnan(output_train)] = 0
    input_test[np.isnan(input_test)] = 0
    output_test[np.isnan(output_test)] = 0

    list1 = minmax(num1.iloc[1:, ].values)
    inputn_train = mapminmax(input_train, list1)
    inputn_test = mapminmax(input_test, list1)
    outputn_train = outputmapminmax(output_train, list1)

    bmin = min(output_train)
    bmax = max(output_train)

    sigmax_list = []
    sigmay_list = []
    sigmaz_list = []
    sigma_list = []
    fit_list = []

    my_pso = PSO(pN=200, dim=4, max_iter=500, w=0.8)
    my_pso.init_Population()
    fitness = my_pso.iterator()
    print("最佳sigma", sigma_list[np.argmin(fit_list)])
    Euclidean_DF_guass = distance(inputn_train, inputn_test, sigma_list[np.argmin(fit_list)][0],
                                  sigma_list[np.argmin(fit_list)][1], sigma_list[np.argmin(fit_list)][2],
                                  sigma_list[np.argmin(fit_list)][3])
    sum_out = sum_layer(Euclidean_DF_guass, outputn_train)
    prediction_result = output_layer(sum_out)
    grnn_output = reverseminmax(prediction_result, bmin, bmax)

    print('结果', output_test, grnn_output)
    grnn_error = mean_squared_error(output_test, grnn_output)
    print('PSO-KMGRNN均方误差', get_mse(output_test, grnn_output))
    print('PSO-KMGRNN均方根误差', get_rmse(output_test, grnn_output))
    print('PSO-KMGRNN平均绝对百分比误差', mape(output_test, grnn_output))
    print('PSO-KMGRNN神经网络预测的均方误差为', grnn_error)
    print('PSO-KMGRNN平均绝对误差', get_mae(output_test, grnn_output))
    r = R2(output_test, grnn_output)
    print('PSO-KMGRNN决定系数', r)

    plt.figure()
    plt.title("PSO-KMGRNN预测结果对比", size=18)
    plt.plot(grnn_output, 'k-*', label='PSO-KMGRNN预测值')
    plt.plot(output_test, 'r-o', label='实际值')
    plt.xlabel('测试样本', size=18)
    plt.ylabel('矿石品位值 %', size=18)
    plt.legend()

    plt.figure()
    plt.plot(output_test, grnn_output, '*', label='实际值-预测值')
    plt.title("PSO-KMGRNN-决定系数", size=18)
    x = np.linspace(min(output_test), max(output_test), 3)
    y = x
    plt.plot(x, y, color='green', label='y=x')
    r = round(r, 6)
    l = 'R^2={}'.format(r)
    plt.text(min(output_test), 1.145, l, fontsize=15)
    plt.legend(fontsize=15)
    plt.xlabel('实际值 %', size=18)
    plt.ylabel('预测值 %', size=18)

    plt.figure()
    plt.plot(grnn_error, 'k-*')
    plt.title("PSO-KMGRNN均方误差")
    plt.ylabel('均方误差')
    plt.show()

    plt.figure(1)
    plt.title("Figure")
    plt.xlabel("iterators", size=14)
    plt.ylabel("fitness", size=14)
    t = np.array([t for t in range(0, 500)])
    fitness = np.array(fitness)
    plt.plot(t, fitness, color='b', linewidth=3)
    plt.show()
